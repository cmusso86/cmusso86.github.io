---
title: "Lista 1"
subtitle: "Gera√ß√£o de NPA‚Äôs (N√∫meros Pseudo-Aleat√≥rios)"
author: 
  Carolina Musso
  251103768
date: today
institute: "Mestrado Acad√™mico em Estat√≠stica - UnB"
lang: pt
format: html
editor: source
execute:
  echo: true
  message: false
  warning: false
---


## Exerc√≠cio 1) 
A distribui√ß√£o Laplace padr√£o tem densidade
$f(x) = \frac{1}{2}e^{-\lvert x \lvert}~,x \in \mathbb{R}$
use o m√©todo da transformada inversa para gerar uma amostra aleat√≥ria de tamanho 1000 dessa distribui√ß√£o (plote um histograma).

### Solu√ß√£o

A distribui√ß√£o Laplace √© uma distribui√ß√£o sim√©trica de caudas pesadas. A fun√ß√£o de densidade de probabilidade √© dada por:

$f(x) = \frac{1}{2b}e^{-\frac{\lvert x-\mu\lvert}{n}}$. A Laplace padr√£o √© um caso espec√≠fico, com $\mu = 0$ e $b = 1$, que resulta na fun√ß√£o de densidade de probabilidade $f(x) = \frac{1}{2}e^{-\lvert x \lvert}$. Dessa forma, ao final desse exerc√≠cio, esperamos que o histograma mostre um comportamento de uma fun√ß√£o sim√©trica, centrada no zero com um pico abrupto em zero.

Primeiramente, precisamos calcular a fun√ß√£o de distribui√ß√£o acumulada da distribui√ß√£o com a integral:

$$
F(x) = \int_{-\infty}^x f(t) \, dt
$$

Para calcular a FDA, precisaremos dividir a integral em duas  partes. Se o x for positivo, a F(x) cobrir√° toda a regi√£o negativa + parte da regi√£o positiva do dom√≠nio. Ou seja, o expoente da parte negativa ser√° invertido para valor positivo (que retornar√° ao negativo com o sinal de - que est√° fora do m√≥dulo), e o da parte positiva ter√° o sinal mantido, que tamb√©m resultar√°n em $-x$ com o sinal exterior ao m√≥dulo. Ou seja, para garantir que o sinal seja sempre negativo ao final, devemos inverter o sinal de x quando ele se encontra na parte negativa. 

J√° se o x for negativo, precisaremos apenas integrar a parte negativa da densidade (ou seja, invertendo o sinal de x).


```{=tex}
\begin{align}
F_{x\geq 0}(x) &= \int_{-\infty}^0 \frac{1}{2} e^{x}dx + \int_{0}^x \frac{1}{2}e^{-x}dx\\
&= \frac{1}{2}(e^0 - e^{-\infty}) + \frac{1}{2}(-e^{-x}-(-e^0))\\
&= \frac{1}{2}(1 - 0) + \frac{1}{2}(1 -e^{-x})\\
&=\frac{1}{2}+\frac{1}{2}(1 - e^{-x}) \\
&=\frac{1}{2}+\frac{1}{2}- e^{-x} \\
& = 1-\frac{e^{-x}}{2}\\
\end{align}
```



```{=tex}
\begin{align}
F_{x \leq 0}(x) &= \int_{-\infty}^x \frac{1}{2} e^{-x}dx\\
&= \frac{1}{2} (e^{x}-e^{-\infty})\\
&= \frac{1}{2} (e^{x}-0)\\
&= \frac{e^x}{2} 
\end{align}
```

Portanto:
$$
F(x) = \begin{cases} 
\frac{1}{2} e^{x} &, ~ \text{se } x < 0 \\
1 - \frac{1}{2} e^{-x} &,~ \text{se } x \geq 0 
\end{cases}
$$

Agora, devemos calcular a inversa da fun√ß√£o de distribui√ß√£o acumulada para usar o teorema da transformada inversa. 

$$
u = F(x)\\
F^{-1}(x)= \begin{cases} 
log(2u) &, ~ \text{se } x < 0, u <0.5  \\
-log(2 -2y) &, ~ \text{se } x \geq 0, u \geq 0.5 \\
\end{cases}
$$

Criemos agora a fun√ß√£o para gerar a amostra aleat√≥ria.

```{r}
#| echo: false

# PACOTES USADOS NA LISTA -----
if(!require("pacman")){ install.packages("pacman")}
pacman::p_load(tidyverse, extraDistr, MASS, rmutil)
```

```{r}
gerador_laplace <- function(n){
  
  u <- runif(n) # gera amostras da uniforme
  x <- ifelse(u < 0.5, # se u < 0.5,
              log(2*u), 
              -log(2-2*u))
  return(x)
}

```

O exerc√≠cio pede que se gere uma amostra de tamanho 1000, para efeito de compara√ß√£o do efeito do tamanho na amostra, gerei amostras de tamanhos diferentes. 


```{r}

## Gerei com tamanhos de amostras diferentes para compara√ß√£o
set.seed(42)
a <- data.frame(amostra = "n = 10^2", lap = gerador_laplace(100))
b <- data.frame(amostra = "n = 10^3", lap = gerador_laplace(1000))
c <- data.frame(amostra = "n = 10^4", lap = gerador_laplace(10000))
d <- data.frame(amostra = "n = 10^5", lap = gerador_laplace(100000))

amostras <- rbind(a, b, c, d) |> 
  mutate(amostra = factor(amostra, levels = c("n = 10^2", 
                                              "n = 10^3", 
                                              "n = 10^4", 
                                              "n = 10^5")))
amostras %>% 
  ggplot(aes(x = lap)) +
  geom_histogram(aes(y = ..density..), bins = 300, 
                 fill = "blue", alpha = 0.5) +
  geom_function(fun = function(x) rmutil::dlaplace(x, m = 0, s = 1)) + 
  labs(title = "Histograma da amostra gerada\n com a distribui√ß√£o Laplace",
       x = "",
       y = "Densidade") +
  theme_minimal() +
  facet_wrap(~amostra) +
  xlim(c(-10, 10))  # Aju


```

Podemos observar que de fato o gerador que criamos partindo do teorema da transformada inversa gera uma amostra que se aproxima da distribui√ß√£o Laplace padr√£o. O histograma mostra um comportamento de uma fun√ß√£o sim√©trica, centrada no zero com um pico abrupto em zero, sendo que para n=100000, o histograma se aproxima mais da fun√ß√£o densidade de probabilidade.

## Exerc√≠cio 2) 

Dado a densidade $f(x|\theta)$ e a densidade a priori $\pi(\theta)$, se observamos $\textbf{x} = x_1, ..., x_n$, a distribui√ß√£o a posteriori de $\theta$ √© $\pi(\theta|\textbf{x}) = \pi(\theta|x_1, ..., x_n) \propto \prod f(x|\pi)\pi(\theta)$
em que $\prod f(x_i|\theta) = L(\theta|x_1,..., x_n)$ √© a fun√ß√£o de verossimilhan√ßa.


Para estimar uma m√©dia normal, uma priori robusta √© a Cauchy. Para $X_i \sim N(\theta,1), \theta \sim Ca(0,1)$, a distribui√ß√£o a posteriori √© $\pi(\theta|\textbf{x})\propto\frac{1}{\pi}\frac{1}{1 + \theta^2}\frac{1}{(2\pi)^{n/2}}\prod_{i = 1}^ne^{-(x_i-\theta)^2/2}$
Seja $\theta = 3, n = 10$ùúÉ, e gere $X_1, ..., X_n ~N(\theta_0,1)$. Use o algoritmo da Aceita√ß√£o-Rejei√ß√£o com uma candidata Cauchy Ca(0,1)) para gerar uma amostra da distribui√ß√£o a posteriori. Avalie qu√£o bem o valor $\theta_0$√© recuperado. Extenda o c√≥digo de maneira que n = 10,25,50,100. Assuma que $M = L(\hat{\theta}|x_1, ..., x_n)$ùëÄ=ùêø(ùúÉÀÜ|ùë•1,‚Ä¶,ùë•ùëõ), ou seja ùëÄ √© a fun√ß√£o de verossimilhan√ßa avaliada no estimador de m√°xima verossimilhan√ßa.

### Solu√ß√£o

A infer√™ncia bayesiana √© uma abordagem poderosa para a estima√ß√£o de par√¢metros, uma vez que, para todos os efeitos, considera-se o par√¢metro como se possu√≠sse uma distribui√ß√£o de probabilidade, diferentemente da infer√™ncia cl√°ssica, que trata o par√¢metro como um valor fixo. Dessa forma, conseguimos medir de fato probabilidades para o par√¢metro, e n√£o intervalos de confian√ßa que, na verdade, parecem ser meio contraintuitivos, j√° que o par√¢metro √© fixo e estamos falando sobre as chances de o intervalo cobrir o par√¢metro, e n√£o de o par√¢metro estar no intervalo.

Entretanto, a distribui√ß√£o a posteriori √© muitas vezes dif√≠cil de calcular diretamente porque exige uma normaliza√ß√£o que envolve uma integral que pode ser analiticamente praticamente imposs√≠vel de resolver e computacionalmente muito custosa. Para resolver esse problema, podemos usar algoritmos para gerar amostras da posteriori. O algoritmo de aceita√ß√£o-rejei√ß√£o, por exemplo, √© capaz de gerar amostras apenas com a densidade proporcional, sem precisar da constante de normaliza√ß√£o. Ele funciona porque a ideia desse m√©todo de amostragem √© nos permitir gerar amostras de uma distribui√ß√£o complexa a partir de uma distribui√ß√£o mais simples.

Neste exerc√≠cio, vamos usar o algoritmo de aceita√ß√£o-rejei√ß√£o para gerar amostras da distribui√ß√£o a posteriori de $\theta$, dado um conjunto de observa√ß√µes $X_1, ..., X_n$, partindo de uma fun√ß√£o candidata Cauchy (que tamb√©m √© a priori). 

Abaixo, vemos o c√≥digo comentado para cria√ß√£o da fun√ß√£o a ser estimada f(x) - posteriori proporcional, a candidata g(x) - Cauchy, a fun√ß√£o de verossimilhan√ßa L(theta|x) para calcular a constante M.

```{r}
set.seed(42)  # Para garantir a reprodutibilidade
theta0 <- 3
n_values <- c(10, 25, 50, 100)

# log-verossimilhan√ßa
log_likelihood <- function(theta, x) {
  sum(dnorm(x, mean = theta, sd = 1, log = TRUE))
}

# Posteriori expl√≠cita (n√£o-normalizada)
posteriori <- function(theta, xi) {
  (1/pi)*(1/(1 + theta^2)) * 
    (1/(2*pi)^(length(xi)/2)) * 
    exp(-sum((xi - theta)^2)/2)
}

# Candidata Cauchy
g <- function(x) dcauchy(x, location = 0, scale = 1)

```
Tendo definido essas fun√ß√µes, a fun√ß√£o geradora de amostras pode ser escrita da seguinte forma, j√° levando em considera√ß√£o a expans√£o para observa√ß√µes de $n = 10$ at√© $n = 100$.

```{r}

# Fun√ß√£o para gerar amostras por aceita√ß√£o-rejei√ß√£o
gerar_amostras <- function(
    n, # observa√ß√µes de x
    N_amostras = 1000 # tamanho da amostra da posteriori) {
  
  # geramos os dados observados
  xi <- rnorm(n, mean = theta0, sd = 1)
  
  # calculamos o estimador de m√°xima verossimilhan√ßa
  theta_hat <- mean(xi)
  
  # calculamos a constante M para o teste
  M <- exp(log_likelihood(theta_hat, xi))
  
  # inicializamos o vetor de amostras
  amostras <- numeric(0)
  
  ## agora defini o tamanho final da amostra como mil
  while (length(amostras) < N_amostras) {
    theta_cand <- rcauchy(1) # gera 1 theta
    u <- runif(1) #gera 1 u
    
    # a f usa os n_values de x e o valor de theta_cand
    f_theta <- posteriori(theta_cand, xi)
    g_theta <- g(theta_cand)
    
    # teste de aceita√ß√£o-rejei√ß√£o em si
    if (u < f_theta / (M * g_theta)) {
      
      # se aceito, adiciona a amostra
      amostras <- c(amostras, theta_cand)
    }
  }
  
  return(amostras)
}

# Gerar amostras para cada n
amostras_lista <- map(n_values, gerar_amostras)
names(amostras_lista) <- paste0("n = ", n_values)

```

Agora, podemos avaliar a recupera√ß√£o do par√¢metro $\theta_0$ com os histogramas das amostras geradas. Vemos que a distribui√ß√£o a priori (com caudas pesadas) domina para $n$ pequeno, mas, conforme o tamanho da amostra aumenta, a distribui√ß√£o a posteriori se aproxima mais da normal, com um pico mais acentuado em torno do valor verdadeiro do par√¢metro.

```{r}
df_amostras <- map2_dfr(amostras_lista, names(amostras_lista), 
                        ~ tibble(amostra = .x, n = .y))

# Calcular estat√≠sticas para cada grupo
stats <- df_amostras %>%
  group_by(n) %>%
  summarise(media_posterior = mean(amostra), .groups = "drop") |> mutate(n = factor(n, levels = c("n = 10", 
                                  "n = 25", 
                                  "n = 50", 
                                  "n = 100")))  

# Criar os gr√°ficos
df_amostras|> 
  mutate(n = factor(n, levels = c("n = 10", 
                                  "n = 25", 
                                  "n = 50", 
                                  "n = 100"))) |>  
ggplot( aes(x = amostra)) +
  geom_histogram(aes(y = ..density..), bins = 50, fill = "gray80", color = "black") +
  geom_vline(data = stats, aes(xintercept = media_posterior), color = "blue", size = 1) +
  geom_vline(xintercept = theta0, color = "red", linetype = "dashed", size = 1) +
  facet_wrap(~n, scales = "free", ncol = 2) +
  labs(title = "Amostras da posterioris para diferentes\n tamanhos n de observa√ß√µes de X ",
       x = expression(theta),
       y = "Densidade") +
  xlim(2,4.5)+
  theme_minimal() +
  theme(strip.text = element_text(size = 12),
        plot.title = element_text(face = "bold", hjust = 0.5)) 
  #scale_x_continuous(expand = expansion(mult = c(0.02, 0.02)))
```


## Exerc√≠cio 3) 

Gere 200 observa√ß√µes aleat√≥rias de uma distribui√ß√£o normal multivariada de dimens√£o 3 com vetor de m√©dias $\mu = (0,1,2)^\top$ e matriz de covari√¢ncia

$$
\Sigma = \begin{bmatrix}
    1.0 & -0.5 & 0.5 \\
    -0.5 & 1.0 & -0.5 \\
    0.5 & -0.5 & 1.0
\end{bmatrix}
$$


Use o m√©todo de decomposi√ß√£o de Cholesky.

### Solu√ß√£o

A distribui√ß√£o conjunta de vari√°veis aleat√≥rias cont√≠nuas X1,X2, X3 √© normal multivariada , denotada por $N_3 \sim (\mu, \Sigma)$ e a fun√ß√£o densidade de probabilidade conjunta √© dada por:

$$
f(x_1, x_2, x_3) = \frac{1}{(2\pi)^{3/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right)
$$

Defininindo esses par√¢metros no R, temos:
```{r}

# vetor de m√©dias
mu <- matrix(
  c(rep(0,200),
  rep(1,200),
  rep(2,200)), 
  nrow = 3, 
  ncol = 200, 
  byrow = TRUE)

# martriz de covari√¢ncia
mat_cov <- matrix(c(1, -0.5, 0.5,
                  -0.5, 1, -0.5,
                  0.5, -0.5, 1), 
                  nrow = 3, 
                  byrow = TRUE)
```

Sabemos que, se temos $X \sim \mathcal{N}(\mu, \Sigma)$, podemos gerar uma amostra de $X$ a partir de uma amostra de $Z \sim \mathcal{N}(0, I)$, aplicando a transforma√ß√£o $X = CZ + \mu$, onde $C$ √© a matriz de Cholesky de $\Sigma$ e $I$ √© a matriz identidade.

A matriz de Cholesky √© uma matriz triangular inferior $C$ tal que $CC^\top = \Sigma$. Para obter essa matriz no R, podemos usar a fun√ß√£o chol(). 

```{r}

# Uma multivariada Padrao Z ~ N(0, I)
norm1 <- rnorm(200)
norm2 <- rnorm(200)
norm3 <- rnorm(200)

Z <- rbind(norm1, norm2, norm3) #cria uma matriz com 3 linhas e 200 colunas              

## Fazendo a decomposi√ß√£o de Cholesky
C <- chol(mat_cov) 

X <- C %*% Z + mu

```


## Exerc√≠cio 4) 

Considere o artigo ‚ÄúBivariate Birnbaum‚ÄìSaunders distribution and associated inference‚Äù (Kundu et al., 2010), dispon√≠vel em PDF, onde os autores apresentam uma formula√ß√£o para a distribui√ß√£o bivariada de Birnbaum‚ÄìSaunders (BVBS). A gera√ß√£o de dados desta distribui√ß√£o √© descrita na equa√ß√£o (8) do artigo. Utilize a parametriza√ß√£o apresentada no artigo para simular 1.000 observa√ß√µes de um vetor aleat√≥rio bivariado $(T_1, T_2)$ com distribui√ß√£o BVBS($\alpha_1 = 0.5, \alpha_2 = 0.5, \beta_1 = 1.0, \beta_2 = 2.0, \rho = 0.7$ ùõΩ. Apresente um gr√°fico de dispers√£o dos dados gerados.

### Solu√ß√£o

O Algoritmo para a gera√ß√£o desses n√∫meros, segundo o artigo √©:

```{ = tex}
\begin{itemize}
  \item \textbf{Step 1:} Generate independent $U_1$ and $U_2$ from $\mathcal{N}(0, 1)$.

  \item \textbf{Step 2:} Compute
  \[
  Z_1 = \frac{\sqrt{1+\rho} + \sqrt{1-\rho}}{2} U_1 + \frac{\sqrt{1+\rho} - \sqrt{1-\rho}}{2} U_2
  \]
  \[
  Z_2 = \frac{\sqrt{1+\rho} - \sqrt{1-\rho}}{2} U_1 + \frac{\sqrt{1+\rho} + \sqrt{1-\rho}}{2} U_2
  \]

  \item \textbf{Step 3:} Obtain
  \[
  T_i = \beta_i \left[ \frac{1}{2} \alpha_i Z_i + \sqrt{\left( \frac{1}{2} \alpha_i Z_i \right)^2 + 1} \right]^2, \quad \text{for } i = 1, 2.
  \]
\end{itemize}

```




```{r}

# Par√¢metros
alpha1 <- 0.5
alpha2 <- 0.5
beta1 <- 1.0
beta2 <- 2.0
rho <- 0.7
n <- 1000

# Matriz de covari√¢ncia para Z1 e Z2
Sigma <- matrix(c(1, rho, rho, 1), ncol = 2)

# Gerar dados normais correlacionados
set.seed(123)  # para reprodutibilidade
Z <- mvrnorm(n, mu = c(0, 0), Sigma = Sigma)

# Transforma√ß√£o para T1 e T2
T1 <- beta1 * ((alpha1 / 2 * Z[, 1] + sqrt((alpha1 / 2 * Z[, 1])^2 + 1))^2)
T2 <- beta2 * ((alpha2 / 2 * Z[, 2] + sqrt((alpha2 / 2 * Z[, 2])^2 + 1))^2)

# Gr√°fico de dispers√£o
plot(T1, T2, main = "Scatter Plot of BVBS Distributed Data", xlab = "T1", ylab = "T2", pch = 19, col = rgb(0.1, 0.2, 0.5, 0.7))

```

