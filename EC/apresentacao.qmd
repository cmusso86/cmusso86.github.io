---
title: "Introdução a Stan "
subtitle: "(e talvez RSample)"
author: "Carolina Musso"
institute: "Estatística Computacional"
format:
  revealjs:
    logo: images/logo.png
    incremental: true 
    theme: dark
    code-block-border-left: "#31BAE9"
    code-block-bg: "#011627"
    code-overflow: wrap
    highlight-style: dracula
    transition: fade
    self-contained: true
knitr:
  opts_chunk:
    echo: true
    warnings: false
editor: visual
---

## Roteiro da apresentação

-   Inferência bayesiana numa casca de noz
    -   Conceitos centrais, vantagens e problemas
-   Introdução STAN
-   Como usar o STAN no R
    -   Rstan, rstanarm, tidymodels
    -   explorar os resultados com ShinySTAN
-   RSample
    -   num sei se vai chegar até aqui...

## Bayesiana x Frequentistas

Uma outra forma de enxergar a probabilidade/incerteza

::: columns
::: {.column width="50%"}
::: {style="color: darkred"}
**Frequentistas**
:::

-   Parâmetro fixo.
-   Amostragens repetidas teoricamente
-   **Intervalo de confiança:** 95% dos intervalos irão conter o parâmetro.
:::

::: {.column width="50%"}
::: {style="color: darkred"}
**Bayesianos**
:::

-   Incerteza sobre o parâmetro
-   v.a. da perspectiva do observador
-   **Intervalo de credibilidade:** Há 95% de chance do parâmetro estar nesse intervalo.
:::
:::

-   [ZedStatistics](https://www.youtube.com/watch?v=Pahyv9i_X2k&t=803s) - Canal bem nteressante com interpretações visuais e intuitivas de estatística.

## Teorema de Bayes

::: columns
::: {.column width="50%"}
![](images/Thomas_Bayes.gif){width="306"}

::: {style="color: darkred"}
**Bayes/Laplace: séc. 18 e 19**
:::
:::

::: {.column width="50%"}
-   $$
    P (A|B) = \frac{P(B|A)*P(A)}{P(B)}
    $$

-   P(B\|A): verossimilhança

-   P(A): incerteza sobre o parâmetro (*priori*)

-   P(B) é a constatante normalizadora
:::
:::

-   Avanços recentes: ganho de poder computacional.

-   [Bayesian Basics, Michael Clark](https://m-clark.github.io/bayesian-basics/)

## Resumindo o raciocínio

$$
\pi(\theta|y) \propto \pi(y|\theta) * \pi(\theta)
$$

$$
posteriori = verossimilhança * priori
$$

-   Consigo atualizar minhas estimativas e interpretar os resultados de forma mais direta

    -   Não tem almoço gratis

        -   Definir *a priori* é subjetivo.

        -   Computar a *posteriori* as vezes é bem difícil

        -   Maldição da dimensionalidade

## Visualizando essa relação

Um exemplo inédito: **lançamento de uma moeda**

-   Acho balenceada, mas em 10 lançamentos teve 7 caras.

![](images/priori_posteriori.png){fig-align="center"}

## Para fazer modelagens bayesianas

-   Escrever um script que defina um modelo bayesiano

-   Conseguir amostrar e medir parâmetros da distribuição posteriori

    -   STAN, 2012 (atualmente v.2.6)

        -   Stanislaw Ulam, matetático polonês que desenvolveu o método de Monte Carlo nos anos 40.

        -   "Sampling Through Adaptive Neighborhoods"

## STAN

-   Software em C++ gratuito de código aberto

    -   <http://mc-stan.org/>, [Guia do Usuário](https://mc-stan.org/docs/stan-users-guide/index.html), [Manual](https://mc-stan.org/docs/reference-manual/includes.html)
    -   Roda em R, Python, Matlab, Julia, Stata

-   Capaz de

    -   Inferência Bayesiana com amostrador MCMC (NUTS, HMC)

    -   Inferência Bayesiana aproximada com "inferência variacional" (ADVI)

    -   Estimativa de máxima verossimilhança penalizada com otimização (L-BFGS)

## Programação probabilística

![](images/Programacao_prob.png){fig-align="center" width="603"}

-   maioria das aplicações estatísticas.

-   Pode-se implementar automaticamente condicionamento.

## STAN

![](images/bibliotecas.png)

-   [**Michael Betancourt**](https://betanalpha.github.io/assets/case_studies/stan_intro.html#1_prologue) **([Material](https://github.com/betanalpha/knitr_case_studies/tree/master/stan_intro))**

## NUTS

-   Usa (ou usava) o amostrador **no-U-turn** ([Hoffman & Gelman, 2014](http://www.stat.columbia.edu/~gelman/research/unpublished/nuts.pdf)), que advém do **Monte Carlo Hamiltoniano** [(Neal, 2011)](https://arxiv.org/pdf/1206.1901.pdf), que é um MCMC generalizado Metropolis.

    -   adaptativo

    -   performa múltiplos passos por iteração para se mover de forma mais eficiente para a posteriori.

    -   altamente escalável

-   [Canal Youtube](https://www.youtube.com/watch?v=k9sH7x8O0Y8)

-   [**Stan:A Probabilistic Programming Language for Bayesian Inference and Optimization**](https://journals.sagepub.com/doi/10.3102/1076998615606113)

## Lógica do STAN

$$
\pi(y,\theta) = \pi(y|\theta) * \pi(\theta)
$$

-   Encontrar a distribuição conjunta

-   A linguagem de modelagem especifica os elementos em blocos:

    -   dados

    -   parâmetros

    -   modelo

-   [Palestra bem completa](https://youtu.be/uSjsJg8fcwY) do Michael Betancour

## Lógica do STAN

Primeiro bloco: espeço observado

```{r, echo=T, eval=F}
data {                      // Data block
  int<lower = 1 >  N;       // Sample size
  int<lower = 1> K;         // Dimension of model matrix
  matrix[N, K] X;           // Model Matrix
  vector[N] y;              // Target variable
}
```

Segundo bloco: parâmetros

```{r, echo=T, eval=F}
parameters {                // Parameters block}
  vector[K] beta;           // Coefficient vector
  real<lower = 0> sigma;    // Error scale, lower bound at 0
}

```

## Lógica do STAN

Terceiro bloco: modelo

```{r, echo=T, eval=F}
model {  
vector[N] mu;
  mu = X * beta;            // Creation of linear predictor
  
  // priors
  beta  ~ normal(0, 10);  
  sigma ~ cauchy(0, 5);
  
  // likelihood
  y ~ normal(mu, sigma);
  }
```

# Tudo junto

```{r eval=F}
data {                      
  int<lower = 1 >  N;       
  int<lower = 1> K;        
  matrix[N, K] X;           
  vector[N] y;              
}
parameters {                
  vector[K] beta;           
  real<lower = 0> sigma;    
}
model {                     
  vector[N] mu;
  mu = X * beta;            
  beta  ~ normal(0, 10);  
  sigma ~ cauchy(0, 5);
  y ~ normal(mu, sigma);
}
```

-   um arquivo **.stan** ou como um **string**

## Código é compilado

-   Declarar as variáveis

-   Os valores específicos serão fornecidos pelos algortimos que avaliam o Stan.

-   O codigo é traduzido pra C++ e compilado.

    -   maior eficiência, sem ter que programar em C++

-   Bibliotecas convertem em um programa executável capaz de avaliar a log-posteriori e a função gradiente

## 

## RSTan: instalar C++ toolchain

R versão \>= 3.4.0 or later, de preferência \>= 4.0.0

Recomenda-se [RStudio](https://www.rstudio.com/) version \>= 1.4.

Necessário configurar o R para ser capaz de compilar código C++ code: [Windows](https://github.com/stan-dev/rstan/wiki/Configuring-C---Toolchain-for-Windows), [Mac,](https://github.com/stan-dev/rstan/wiki/Configuring-C---Toolchain-for-Mac) [Linux](https://github.com/stan-dev/rstan/wiki/Configuring-C-Toolchain-for-Linux).

```{r eval=F}
install.packages("rstan", repos = "https://cloud.r-project.org/", 
                 dependencies = TRUE)

library("rstan")
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

[Exemplos](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started)

# Mão na massa

## Simular alguns dados ...

```{r }
set.seed(42)
N <- 1000; 
K = 1  # se quiser colocar mais covariaveis
coefs = c(5, .2); sigma = 2

covariates = replicate(
  K, 
  rnorm(n = N)) # se quiser colocar mais covariaveis

x <- rnorm(n = N); X = cbind(Intercept = 1,covariates)

mu = X %*% coefs
y <- rnorm(N, mu, sigma)

```

## Usando lm()

```{r}
modlm = lm(y ~ ., data = data.frame(X[, -1]))
summary(modlm)
```

## Usando RStan

Dados devem ser uma lista!

```{r eval=F}
X = cbind(Intercept = 1,x)
dados_stan = list(
  N = N,
  K = ncol(X),
  y = y,
  X = X
)

# outra opção
dados_stan <- read_rdump("dados.R")
```

## Ajustando o modelo

```{r eval=F}

fit_Rstan <- stan(
  model_code = stanmodelcode, # é um objeto no R criado com uma string 
  data   = dados_stan
)

# OU

fit_Rstan <- stan(
  file='fit_data.stan', # o programa stan está em um arquivo separado
  data   = dados_stan)

```

Vai demorar um pouquinho ...

```{r echo=FALSE, warning=FALSE, message=FALSE}
rm(list=ls())
pacman::p_load(rstan, tidyverse)
set.seed(42)
N <- 1000; 
coefs = c(5, .2); sigma = 2

x <- rnorm(n = N); X = cbind(Intercept = 1,x)

mu = X %*% coefs
y <- rnorm(N, mu, sigma)

# Rstan
X = cbind(Intercept = 1,x)
dados_stan = list(
  N = N,
  K = ncol(X),
  y = y,
  X = X
)


stanmodelcode = "
data {                      // Data block
  int<lower = 1 >  N;       // Sample size
  int<lower = 1> K;         // Dimension of model matrix
  matrix[N, K] X;           // Model Matrix
  vector[N] y;              // Target variable
}

parameters {                // Parameters block
  vector[K] beta;           // Coefficient vector
  real<lower = 0> sigma;    // Error scale, lower bound at 0
}

model {                     // Model block
  vector[N] mu;
  mu = X * beta;            // Creation of linear predictor
  
  // priors
  beta  ~ normal(0, 10);  
  sigma ~ cauchy(0, 5);
  
  // likelihood
  y ~ normal(mu, sigma);
}
"


### Run the model and examine results

fit_Rstan <- stan(
  model_code = stanmodelcode,
  data   = dados_stan
)
```

## RStan

```{r}
print(
  fit_Rstan,
  pars   = c('beta', 'sigma'),
  digits = 3,
  prob   = c(.025, .5, .975)
)
```

## Alguns parametros interessantes

```{r eval=F}
stan(..., 
  pars = NA, #se quiser salavr só alguns parâmetros
  chains = 4, # numero de cadeiras paralelsas
  iter = 2000, # numero de iterações
  warmup = floor(iter/2), # burnin
  thin = 1, # thining (período para se salvar amostras)
  init = "random", # valores iniciais
  seed = sample.int(.Machine$integer.max, 1), 
  algorithm = c("NUTS", "HMC", "Fixed_param"),
  ...)
```

## Usando rstanarm

Uma interface da interface?

```{r echo=F}
pacman::p_load(rstan, tidyverse)
set.seed(42)
N <- 1000; 
coefs = c(5, .2); sigma = 2

x <- rnorm(n = N);X = cbind(Intercept = 1,x)

mu = X %*% coefs
y <- rnorm(N, mu, sigma)

dados_stanarm <- data.frame(
  x=x, y=y
)
```

```{r, message=FALSE, warning=FALSE, results='hide' }
pacman::p_load(rstanarm)

beta_prior <- rstanarm::normal(0,10); sigma_prior <- rstanarm::cauchy(0,5)

fit_rstanarm<-rstanarm::stan_glm(y~., data=data.frame(X[, -1]), 
                         prior=beta_prior,
                         prior_aux = sigma_prior)

```

### Coeficientes

```{r}
fit_rstanarm$coefficients
```

## Tidymodels

"The [tidymodels](https://www.tidymodels.org) framework is a collection of packages for modeling and machine learning using [**tidyverse**](https://www.tidyverse.org/) principles"

![](images/tidymodel.jpg){fig-align="center" width="611"}

## Com o tidymodels

```{r}
library(tidymodels)

bayes_mod <-   
  linear_reg() %>% 
  set_engine("stan", 
             prior_intercept = beta_prior, 
             prior = beta_prior,
             prior_aux = sigma_prior) 

bayes_fit <- 
  bayes_mod %>% 
  fit(y ~ x, data = data.frame(X[, -1]))

print(bayes_fit, digits = 5)

```

## Shinystan

```{r eval=F}
pacman::p_load(shinystan)

avaliando_modelo <- launch_shinystan(fit_Rstan)

```

## Outras referências

[Um estudo de caso, Rbloggers](https://www.r-bloggers.com/2019/05/bayesian-modeling-using-stan-a-case-study/), [Outro](Applied%20Bayesian%20Statistics%20Using%20Stan%20and%20R), [Stan for epdemiology](https://epidemiology-stan.github.io)

[Become a Baysian with R and Stan](https://m-clark.github.io/workshops/bayesian/), [Bayesian data analysis - RStan demos](http://avehtari.github.io/BDA_R_demos/demos_rstan/rstan_demo.html)

[R/Stan examples](https://hedibert.org/wp-content/uploads/2021/02/stan-rstan-examples.html), [Stan function references](https://mc-stan.org/docs/functions-reference/index.html)

[Prior distributions for RStan models](https://cran.r-project.org/web/packages/rstanarm/vignettes/priors.html)

[Materiais da StanCon](https://github.com/stan-dev/stancon_talks), [Lista de Estudos de Casos - oficial](https://mc-stan.org/users/documentation/case-studies.html)

[Towards A Principled Bayesian Workflow](https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html#111_Quantifying_Consequences)

[Bayesian Modeling Using Stan](https://bayesball.github.io/BRMS/)

# RSample

Dá tempo?

## Um dos pacotes do tidymodels é o RSample

O pacote [rsample](https://rsample.tidymodels.org/index.html) fornece funções para criar diferentes tipos de reamostragens com o intuito de:

-   reamostrar para estimar a distribuição de uma estatística.

<!-- -->

-   estimar a perfomance de um modelo usando um conjunto de validação

![](images/boot.png){fig-align="center"}

## Rsample

Os datasets criados são diretamente acessíveis no objeto mas sem consumir tanta memória.

```{r}
library(rsample)
library(mlbench)

data(LetterRecognition)
lobstr::obj_size(LetterRecognition)

set.seed(35222)
boots <- bootstraps(LetterRecognition, times = 50)
lobstr::obj_size(boots)

# Object size per resample
lobstr::obj_size(boots)/nrow(boots)

# Fold increase is <<< 50
as.numeric(lobstr::obj_size(boots)/lobstr::obj_size(LetterRecognition))
```

## Rsample

### Objeto da classe `rset`\`

```{r}
library(rsample)
set.seed(8584)
bt_resamples <- bootstraps(mtcars, times = 3)
bt_resamples
```

## Cada amostra

```{r}
first_resample <- bt_resamples$splits[[1]]
first_resample
#> <Analysis/Assess/Total>
#> <32/14/32>

as.data.frame(first_resample)

```

## Analysis e Assessment

```{r}
head(analysis(first_resample))
head(assessment(first_resample))

```

## Intervalo de confiança com Bootstrap

```{r fig.align="center"}
pacman::p_load(nlstools,GGally)
data(O2K)

ggplot(O2K, aes(x = t, y = VO2)) + 
  geom_point()+
  theme_bw(base_size = 14)
```

## O modelo

```{r}
nonlin_form <-  
  as.formula(
    VO2 ~ (t <= 5.883) * VO2rest + 
      (t > 5.883) * 
      (VO2rest + (VO2peak - VO2rest) * (1 - exp(-(t - 5.883) / mu)))
    )

# valores iniciais por inspeção visual
start_vals <- list(VO2rest = 400, VO2peak = 1600, mu = 1)

res <- nls(nonlin_form, start = start_vals, data = O2K) 

tidy(res)
```

## Usando cada amostra

```{r warning=FALSE, message=FALSE}
# função para ajutar cada amostra ao modelo 
fit_fun <- function(split, ...) {
  nls(nonlin_form, data = analysis(split), ...) %>%
    tidy()
}

set.seed(462)
nlin_bt <-
  bootstraps(O2K, times = 2000, apparent = TRUE) %>%
  mutate(models = map(splits, ~ fit_fun(.x, start = start_vals)))
nlin_bt
nlin_bt$models[[1]]
```

## Obserservando os resultados

```{r warning=FALSE, message=FALSE}
nls_coef <- 
  nlin_bt %>%
  dplyr::select(-splits) %>%
  # empilhar as colunas 
  unnest() %>%
  dplyr::select(id, term, estimate) 
head(nls_coef)
```

## Só os histogramas

```{r fig.align='center'}
p_ints <- int_pctl(nlin_bt, models)
nls_coef %>% 
  ggplot(aes(x = estimate)) + 
  geom_histogram(bins = 20, col = "white") + 
  facet_wrap(~ term, scales = "free_x") + 
  geom_vline(data = p_ints, aes(xintercept = .lower), col = "red") + 
  geom_vline(data = p_ints, aes(xintercept = .upper), col = "red")
```

# Obrigada!

só de pensar que ainda tem uma lista pra entregar...
